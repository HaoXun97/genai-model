import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import textwrap
import chromadb
from sentence_transformers import SentenceTransformer
from unsloth import FastLanguageModel

# Configuration
PERSIST_DIR = "chroma_db"
COLLECTION_NAME = "legislation_whitepaper_embeddings"
EMBED_MODEL_NAME = "BAAI/bge-m3"  # same as your indexing
TOP_K = 3  # number of retrieved passages to include in prompt
MAX_CHARS_PER_DOC = 1200  # truncate each retrieved doc to avoid too long prompts

# Device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load embedding model
print(f"Loading embedding model {EMBED_MODEL_NAME} on device={device}...")
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=device)
print("Embedding model loaded.")

# Open Chroma DB
client = chromadb.PersistentClient(path=PERSIST_DIR)
collection = client.get_or_create_collection(COLLECTION_NAME)

if collection.count() == 0:
    print("âš ï¸  æ³¨æ„ï¼šChroma é›†åˆç›®å‰ç‚ºç©ºã€‚è«‹å…ˆåŸ·è¡Œ `python rag.py` ä¾†å»ºç«‹ç´¢å¼•ï¼Œæˆ–ç¢ºä¿ `chroma_db` è³‡æ–™å¤¾å­˜åœ¨ä¸”å·²åŒ…å«å‘é‡ã€‚")

# Helper: retrieve top-k docs
def retrieve(query: str, top_k: int = TOP_K):
    emb = embed_model.encode([query], convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)[0]
    n_results = min(collection.count(), top_k) if collection.count() > 0 else 0
    if n_results == 0:
        return []
    results = collection.query(
        query_embeddings=[emb.tolist()],
        n_results=n_results,
        include=["documents", "metadatas", "distances"]
    )

    docs = []
    docs_list = results.get("documents", [[]])[0]
    metas_list = results.get("metadatas", [[]])[0]
    dists_list = results.get("distances", [[]])[0]

    for doc, meta, dist in zip(docs_list, metas_list, dists_list):
        docs.append({
            "doc": doc,
            "meta": meta,
            "distance": dist
        })
    return docs

# Build a prompt that includes retrieved context and the user question
def build_prompt(query: str, retrieved_docs: list):
    if not retrieved_docs:
        ctx = "(æœªæ‰¾åˆ°ç›¸é—œæ³•æ¢æˆ–æ®µè½)"
    else:
        parts = []
        for i, r in enumerate(retrieved_docs, start=1):
            meta = r.get("meta", {})
            header = meta.get("header") or "(ç„¡æ¨™é¡Œ)"
            src = f"{meta.get('file', 'unknown')} (pages {meta.get('start_page')} - {meta.get('end_page')})  æ¨™é¡Œ: {header}"
            # truncate doc text
            text = r.get("doc", "").strip()
            if len(text) > MAX_CHARS_PER_DOC:
                text = text[:MAX_CHARS_PER_DOC] + "..."
            part = f"ã€ä¾†æº {i}ã€‘ {src}\n{text}"
            parts.append(part)
        ctx = "\n\n---\n\n".join(parts)

    prompt = textwrap.dedent(f"""
    Human: ä»¥ä¸‹æ˜¯å¾æ³•è¦è³‡æ–™åº«æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹ï¼Œè«‹åƒè€ƒä¸¦ä¸”æ ¹æ“šé€™äº›å…§å®¹å›ç­”ä¸‹é¢çš„å•é¡Œï¼›å›ç­”å¾Œè«‹ç°¡çŸ­åˆ—å‡ºä½ å¼•ç”¨çš„ä¾†æº (æª”åã€èµ·è¨–é ã€æ¨™é¡Œ)ã€‚

    {ctx}

    å•é¡Œ: {query}

    æ‚Ÿç©º:
    """)
    return prompt

# Load LLM for inference
print("\nLoading fine-tuned model for inference...")
MODEL_PATH = "./bart_finetuned"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_PATH,
    max_seq_length=1024,
    dtype=torch.bfloat16,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)
print("Model loaded. ä½ å¯ä»¥é–‹å§‹è¼¸å…¥å•é¡Œï¼Œè¼¸å…¥ /exit çµæŸã€‚\n")

# Interactive loop

def interactive_loop():
    while True:
        try:
            user_input = input("You: ")
        except KeyboardInterrupt:
            print("\nBye!")
            return
        if user_input.strip().lower() == "/exit":
            print("Bye!")
            break
        if not user_input.strip():
            continue

        # safety flag
        skip = False

        # Retrieve
        retrieved = retrieve(user_input, TOP_K)

        # Build prompt
        prompt = build_prompt(user_input, retrieved)

        # Tokenize with safety: ensure prompt is string, enable truncation and fallback on errors
        if not isinstance(prompt, str):
            prompt = str(prompt)
        try:
            max_len = getattr(tokenizer, "model_max_length", None) or getattr(tokenizer, "max_length", None) or 4096
            truncation_max = max(32, max_len - 64)
            inputs = tokenizer(
                text=prompt,
                images=None,
                return_tensors="pt",
                truncation=True,
                max_length=truncation_max,
            ).to(model.device)
        except TypeError as e:
            print(f"âš ï¸ Tokenizer TypeError: {e}. åªä½¿ç”¨ç°¡åŒ– prompt ä¸¦é‡è©¦ã€‚")
            safe_prompt = f"Human: {user_input}\n\næ‚Ÿç©º:"
            try:
                inputs = tokenizer(
                    text=safe_prompt,
                    images=None,
                    return_tensors="pt",
                    truncation=True,
                    max_length=min(256, getattr(tokenizer, "model_max_length", 1024)-1),
                ).to(model.device)
            except Exception as e2:
                print(f"âš ï¸ ç„¡æ³• tokenizedï¼Œå³åˆ»è·³éï¼š{e2}")
                skip = True
        except Exception as e:
            print(f"âš ï¸ Tokenizer éŒ¯èª¤ï¼š{e}ã€‚è·³éæ­¤è¼¸å…¥ã€‚")
            continue

        if skip:
            continue

        # Generate â€” use max_new_tokens to avoid ValueError when input length already equals max_length
        max_new_tokens = 256  # adjust as needed
        try:
            max_pos = getattr(model.config, "max_position_embeddings", None)
            input_len = inputs["input_ids"].shape[1]
            if max_pos is not None:
                max_new_tokens = min(max_new_tokens, max_pos - input_len - 1)
                if max_new_tokens < 1:
                    max_new_tokens = 32
        except Exception:
            pass

        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    eos_token_id=tokenizer.eos_token_id,
                )
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}ã€‚å˜—è©¦ç”¨ç°¡çŸ­æç¤ºé‡è©¦ã€‚")
            try:
                safe_prompt = f"Human: {user_input}\næ‚Ÿç©º:"
                inputs = tokenizer(text=safe_prompt, images=None, return_tensors="pt", truncation=True, max_length=256).to(model.device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=128,
                        temperature=0.7,
                        do_sample=True,
                        top_p=0.9,
                        repetition_penalty=1.1,
                        eos_token_id=tokenizer.eos_token_id,
                    )
            except Exception as e2:
                print(f"âš ï¸ ä»ç„¡æ³•ç”¢ç”Ÿå›æ‡‰ï¼š{e2}ã€‚è·³éæ­¤è¼¸å…¥ã€‚")
                skip = True

        if skip:
            continue

        # Decode safely
        try:
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            print(f"âš ï¸ è§£ç¢¼å›æ‡‰æ™‚å‡ºéŒ¯ï¼š{e}ã€‚è·³éæ­¤è¼¸å…¥ã€‚")
            continue

        response_part = response.split(prompt)
        if len(response_part) > 1:
            answer = response_part[1].strip()
        else:
            answer = response.split("æ‚Ÿç©º:")[-1].strip()

        print(f"\næ‚Ÿç©º: {answer}\n")

        # Print retrieved sources for transparency
        if retrieved:
            print("ğŸ” æª¢ç´¢åˆ°çš„ä¾†æºï¼š")
            for i, r in enumerate(retrieved, start=1):
                meta = r.get("meta", {})
                print(f" {i}. {meta.get('file', 'unknown')} | pages {meta.get('start_page')} | æ¢æ–‡: {meta.get('header')}")
            print("")

if __name__ == "__main__":
    interactive_loop()
