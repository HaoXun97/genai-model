{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf244eb-f819-467c-a8e5-f8d8e8f7c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello World Demo for Llama 3.2 11B Vision-Instruct\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1. Install required libraries if not already installed\n",
    "#!pip install transformers accelerate safetensors torch pillow --upgrade\n",
    "\n",
    "# 2. Import dependencies\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# 3. Set model path (local download)\n",
    "model_path = \"Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# 4. Load processor (handles text + vision input) and model\n",
    "processor = AutoProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",       # Automatically uses GPU if available\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# 5. Create a simple Hello World prompt\n",
    "prompt = \"Hello Llama! Can you introduce yourself in one sentence?\"\n",
    "\n",
    "# 6. Preprocess input\n",
    "inputs = processor(text=prompt, images=None, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 7. Generate output\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# 8. Decode result\n",
    "response = processor.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "print(\"Model Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e60b20-1119-456a-8aad-83b010cd5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "MODEL_PATH = os.environ.get(\"LLAMA_VISION_PATH\", \"/home/jj/Llama-3.2-11B-Vision-Instruct\")\n",
    "USE_BF16 = torch.cuda.is_available()\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "# ---------------------------\n",
    "# Load model & processor\n",
    "# ---------------------------\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16 if USE_BF16 else torch.float32,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------\n",
    "# Conversation state\n",
    "# ---------------------------\n",
    "history: List[Dict[str, Any]] = []\n",
    "SYSTEM_PROMPT = \"You are a helpful, concise assistant.\"\n",
    "\n",
    "def build_inputs(\n",
    "    history: List[Dict[str, Any]],\n",
    "    user_text: Optional[str] = None,\n",
    "    user_images: Optional[List[Any]] = None,\n",
    ") -> Tuple[Dict[str, torch.Tensor], Optional[List[Any]]]:\n",
    "    \"\"\"\n",
    "    Build chat-formatted prompt + image bundle using processor.apply_chat_template.\n",
    "    \"\"\"\n",
    "    conversation = []\n",
    "    # optional system instruction\n",
    "    conversation.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "    # Find all images from conversation history\n",
    "    all_images = []\n",
    "    \n",
    "    # prior turns\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            if turn.get(\"images\"):\n",
    "                conversation.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": turn[\"content\"]}] +\n",
    "                               [{\"type\": \"image\"} for _ in turn[\"images\"]],\n",
    "                })\n",
    "                # Collect images for the processor\n",
    "                all_images.extend(turn[\"images\"])\n",
    "            else:\n",
    "                conversation.append({\"role\": \"user\", \"content\": turn[\"content\"]})\n",
    "        else:\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": turn[\"content\"]})\n",
    "\n",
    "    # current user turn\n",
    "    if user_text is not None:\n",
    "        if user_images:\n",
    "            conversation.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": user_text}] +\n",
    "                           [{\"type\": \"image\"} for _ in user_images],\n",
    "            })\n",
    "            all_images.extend(user_images)\n",
    "        else:\n",
    "            conversation.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # Apply chat template\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    # For Llama 3.2 Vision, we need to pass ALL images that appear in the conversation\n",
    "    # even if they were from previous turns\n",
    "    final_images = all_images if all_images else None\n",
    "    \n",
    "    # Processor packs text+images to tensors\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=final_images,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    return inputs, final_images\n",
    "\n",
    "def generate_stream(inputs: Dict[str, torch.Tensor]) -> str:\n",
    "    \"\"\"\n",
    "    Stream tokens to stdout during generation. Returns the finalized assistant text.\n",
    "    \"\"\"\n",
    "    streamer = TextIteratorStreamer(processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    \n",
    "    # Extract generation arguments - don't pass processor inputs directly\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "    \n",
    "    # Add vision inputs if present\n",
    "    if \"pixel_values\" in inputs:\n",
    "        generation_kwargs[\"pixel_values\"] = inputs[\"pixel_values\"]\n",
    "    if \"aspect_ratio_ids\" in inputs:\n",
    "        generation_kwargs[\"aspect_ratio_ids\"] = inputs[\"aspect_ratio_ids\"]\n",
    "    if \"aspect_ratio_mask\" in inputs:\n",
    "        generation_kwargs[\"aspect_ratio_mask\"] = inputs[\"aspect_ratio_mask\"]\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # stream to terminal\n",
    "    sys.stdout.write(\"Assistant: \")\n",
    "    sys.stdout.flush()\n",
    "    chunks = []\n",
    "    for token_text in streamer:\n",
    "        sys.stdout.write(token_text)\n",
    "        sys.stdout.flush()\n",
    "        chunks.append(token_text)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    return \"\".join(chunks).strip()\n",
    "\n",
    "def main():\n",
    "    print(\"=== Llama 3.2 11B Vision-Instruct — Interactive Chat ===\")\n",
    "    print(\"Tips:\")\n",
    "    print(\"  • Type your question and press Enter.\")\n",
    "    print(\"  • Type '/img path1 [path2 ...]' to load images, then ask a question.\")\n",
    "    print(\"  • Type '/quit' to exit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user = input(\"You: \").strip()\n",
    "        if user.lower() in {\"/quit\", \"quit\", \"exit\"}:\n",
    "            break\n",
    "\n",
    "        # Handle image loading\n",
    "        if user.startswith(\"/img\"):\n",
    "            # Lazily import PIL\n",
    "            try:\n",
    "                from PIL import Image\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Pillow not available ({e}). Try: pip install pillow\")\n",
    "                continue\n",
    "\n",
    "            paths = user.split()[1:]\n",
    "            if not paths:\n",
    "                print(\"Usage: /img /path/to/img1 [/path/to/img2 ...]\")\n",
    "                continue\n",
    "                \n",
    "            imgs = []\n",
    "            for p in paths:\n",
    "                try:\n",
    "                    imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "                    print(f\"Loaded: {p}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not open {p}: {e}\")\n",
    "            if not imgs:\n",
    "                print(\"No valid images loaded.\")\n",
    "                continue\n",
    "                \n",
    "            # Ask for a text question next\n",
    "            q = input(\"Image(s) loaded. Ask your first question about them:\\nYou: \").strip()\n",
    "            if not q:\n",
    "                print(\"No question provided.\")\n",
    "                continue\n",
    "            \n",
    "            # Build + stream with images\n",
    "            try:\n",
    "                inputs, _ = build_inputs(history, user_text=q, user_images=imgs)\n",
    "                answer = generate_stream(inputs)\n",
    "                history.append({\"role\": \"user\", \"content\": q, \"images\": imgs})\n",
    "                history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Normal turn (text-only, but may reference previous images)\n",
    "        try:\n",
    "            inputs, _ = build_inputs(history, user_text=user, user_images=None)\n",
    "            answer = generate_stream(inputs)\n",
    "            history.append({\"role\": \"user\", \"content\": user})\n",
    "            history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            print(\"Try loading images first with '/img /path/to/image.jpg'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wukong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
