{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5dcb19-5d82-4495-bebb-4625e791d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1: Informal language fine-tuning ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4613be561a3146f3a8e908cdd410a1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733493a838074450be709d7d347af2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new PEFT model\n",
      "trainable params: 4,259,840 || all params: 9,779,451,920 || trainable%: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  2.91it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 2.4602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  3.04it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  2.98it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 0.3014\n",
      "\n",
      "--- Stage 2: Bart Simpson-specific fine-tuning ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f0ba23b3d3459ea9b0c3451fdadce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b3f2d34fec4aef96df96ef244fd4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new PEFT model\n",
      "trainable params: 4,259,840 || all params: 9,779,451,920 || trainable%: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:29<00:00,  3.02it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 1.2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:29<00:00,  3.00it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:29<00:00,  3.03it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:29<00:00,  3.03it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.2275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:29<00:00,  3.02it/s, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.2011\n",
      "\n",
      "Fine-tuning completed. Final model saved to: ./bart_finetuned\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "def prepare_dataset(file_path, tokenizer, max_length=256):\n",
    "    \"\"\"Loads a JSONL file and tokenizes it for causal language modeling.\"\"\"\n",
    "    dataset = load_dataset(\"json\", data_files=file_path)[\"train\"]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        if \"prompt\" not in examples or \"completion\" not in examples:\n",
    "            raise KeyError(\"Dataset must have 'prompt' and 'completion' fields.\")\n",
    "\n",
    "        # Combine prompt and completion for Causal LM fine-tuning\n",
    "        full_text = [\n",
    "            f\"Human: {p}\\nBart:{c}{tokenizer.eos_token}\"\n",
    "            for p, c in zip(examples[\"prompt\"], examples[\"completion\"])\n",
    "        ]\n",
    "        model_inputs = tokenizer(\n",
    "            full_text, truncation=True, max_length=max_length, padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Create labels and mask the prompt portion\n",
    "        labels = torch.tensor(model_inputs[\"input_ids\"])\n",
    "        prompts_only = [f\"Human: {p}\\nBart:\" for p in examples[\"prompt\"]]\n",
    "        prompt_toks = tokenizer(prompts_only, add_special_tokens=False)\n",
    "        prompt_lengths = [len(p) for p in prompt_toks[\"input_ids\"]]\n",
    "\n",
    "        for i, length in enumerate(prompt_lengths):\n",
    "            labels[i, :length] = -100  # Mask prompt tokens\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=dataset.column_names\n",
    "    )\n",
    "    tokenized_dataset.set_format(type=\"torch\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "def load_model(model_name, peft_config, peft_model_path=None):\n",
    "    \"\"\"Loads the base model with 4-bit quantization and applies PEFT adapters.\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    if peft_model_path and os.path.exists(os.path.join(peft_model_path, \"adapter_config.json\")):\n",
    "        print(f\"Loading PEFT adapters from {peft_model_path}\")\n",
    "        model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    else:\n",
    "        print(\"Creating new PEFT model\")\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune(model, dataset, output_dir, num_epochs, batch_size=1, learning_rate=1e-4):\n",
    "    \"\"\"Performs the training loop for the PEFT model.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = bnb.optim.PagedAdamW8bit(model.parameters(), lr=learning_rate)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=100, num_training_steps=len(dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Data is on CPU, must be moved to the model's device (GPU)\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        model.save_pretrained(os.path.join(output_dir, f\"checkpoint-epoch-{epoch+1}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the two-stage fine-tuning process.\"\"\"\n",
    "    model_name = \"/home/jj/Llama-3.2-11B-Vision-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Stage 1: Informal language fine-tuning\n",
    "    informal_model_path = \"./informal_finetuned\"\n",
    "    if not os.path.exists(os.path.join(informal_model_path, \"adapter_model.safetensors\")):\n",
    "        print(\"--- Stage 1: Informal language fine-tuning ---\")\n",
    "        informal_dataset = prepare_dataset(\"informal.jsonl\", tokenizer)\n",
    "        model = load_model(model_name, peft_config)\n",
    "        model = fine_tune(model, informal_dataset, informal_model_path, num_epochs=3)\n",
    "    else:\n",
    "        print(\"Skipping Stage 1: Informal fine-tuned model already exists\")\n",
    "\n",
    "    # Stage 2: Bart Simpson-specific fine-tuning\n",
    "    print(\"\\n--- Stage 2: Bart Simpson-specific fine-tuning ---\")\n",
    "    bart_dataset = prepare_dataset(\"bart.jsonl\", tokenizer)\n",
    "    bart_model_path = \"./bart_finetuned\"\n",
    "    model = load_model(model_name, peft_config, informal_model_path)\n",
    "    model = fine_tune(model, bart_dataset, bart_model_path, num_epochs=5)\n",
    "\n",
    "    model.save_pretrained(bart_model_path)\n",
    "    tokenizer.save_pretrained(bart_model_path)\n",
    "    print(\"\\nFine-tuning completed. Final model saved to:\", bart_model_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad5f8b-02c3-4c9c-906c-0d98c0dad8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2f61f8f18248178632aca0c749b56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ay, caramba! Model loaded. Ask me anything, man. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  what is storage?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bart: Storage? It's like... having a really big treehouse for your stuff, dude. You can hide anything in there and forget about it. But, you know, for things instead of toys. Unless you're me, then it's all about toys... and chocolate... in storage. D'oh!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"Loads the 4-bit quantized model and tokenizer from a PEFT checkpoint.\"\"\"\n",
    "    # Step 1: Load the PEFT config to get the base model name\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    base_model_name = config.base_model_name_or_path\n",
    "\n",
    "    # Step 2: Setup 4-bit quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # Step 3: Load the base model with quantization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Load the PEFT model by combining the base model and adapters\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generates a response from the model based on a user prompt.\"\"\"\n",
    "    # Format the prompt to match the training data\n",
    "    full_prompt = f\"Human: {prompt}\\nBart:\"\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode and extract only the new, generated tokens\n",
    "    response = tokenizer.decode(output_ids[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load the model and start a chat session.\"\"\"\n",
    "    model_path = \"./bart_finetuned\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model path not found at {model_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading fine-tuned model...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    \n",
    "    print(\"\\nAy, caramba! Model loaded. Ask me anything, man. Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = generate_response(model, tokenizer, user_input)\n",
    "        print(f\"Bart: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
